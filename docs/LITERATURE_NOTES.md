# Literature Review: Neural Perceptual Audio Mastering

Comprehensive review of research papers that informed this project.

---

## üìö Table of Contents

1. [Differentiable IIR Filters (Nercessian et al. 2020)](#1-differentiable-iir-filters)
2. [Multi-scale STFT Loss (Koo et al. 2022)](#2-multi-scale-stft-loss)
3. [Perceptual Audio Evaluation (Wright & V√§lim√§ki 2016)](#3-perceptual-audio-evaluation)
4. [Temporal Convolutional Networks (Comunit√† et al. 2025)](#4-temporal-convolutional-networks)
5. [Wave-U-Net Architecture (Stoller et al. 2018)](#5-wave-u-net-architecture)
6. [auraloss Framework (Steinmetz & Reiss 2021)](#6-auraloss-framework)
7. [Loudness Standards (ITU-R BS.1770, ISO 226)](#7-loudness-standards)

---

## 1. Differentiable IIR Filters

**Paper:** "Differentiable IIR Filters for Machine Learning Applications"  
**Authors:** Nercessian, S., Panetta, K., & Agaian, S.  
**Year:** 2020  
**Impact:** (Critical - enables our parametric decoder)

### Key Contribution

This paper solved a fundamental problem: **how to make traditional DSP filters trainable via gradient descent**. Prior to this work, using explicit filter coefficients in neural networks was impossible because:
1. Filter outputs weren't differentiable with respect to parameters
2. Backpropagation through filter state was unstable
3. Frequency-domain operations didn't preserve gradients

**Their solution:** Implement biquad filters (second-order IIR) in a way that maintains gradient flow.

### Core Concepts

#### Biquad Filter Structure
A biquad (bi-quadratic) filter is the fundamental building block of most audio EQs:

```
         b0 + b1*z^-1 + b2*z^-2
H(z) = ------------------------
         a0 + a1*z^-1 + a2*z^-2
```

Where:
- `b0, b1, b2` = numerator coefficients (feedforward)
- `a0, a1, a2` = denominator coefficients (feedback)
- `z^-1` = one sample delay

#### Parametric EQ Mapping
The paper shows how to map intuitive parameters to biquad coefficients:

**Parametric EQ Parameters:**
- `fc` = center frequency (Hz)
- `G` = gain (dB)
- `Q` = quality factor (bandwidth control)

**Coefficient Formulas:**
```python
A = 10^(G/40)  # Convert dB to linear
omega = 2 * œÄ * fc / fs
alpha = sin(omega) / (2 * Q)

# For peaking filter:
b0 = 1 + alpha * A
b1 = -2 * cos(omega)
b2 = 1 - alpha * A
a0 = 1 + alpha / A
a1 = -2 * cos(omega)
a2 = 1 - alpha / A
```

#### Differentiability
**Key insight:** If we parametrize the filter by `(fc, G, Q)` instead of raw coefficients, gradients flow naturally:

```
‚àÇL/‚àÇfc = ‚àÇL/‚àÇy * ‚àÇy/‚àÇb * ‚àÇb/‚àÇfc
```

All these derivatives exist and are computable!

### How We Use It

**File:** `src/models.py` - `SimpleBiquadEQ` class (lines 69-120)

```python
class SimpleBiquadEQ(nn.Module):
    def forward(self, audio, center_freqs, gains, q_factors):
        # Uses torchaudio.transforms.BandBiquad
        # This implements Nercessian's differentiable filtering
        eq_filter = torchaudio.transforms.BandBiquad(
            sample_rate=self.sample_rate,
            central_freq=freq.item(),
            Q=q.item(),
            gain=gain.item()
        )
        output = eq_filter(audio)
```

**Why it matters for our project:**
- Enables **white-box parametric path** (interpretable EQ)
- Network learns `(fc, G, Q)` ‚Üí gradients update these parameters
- Professional engineers can inspect/adjust learned EQ curves
- Differentiable = end-to-end training with reconstruction loss

### Limitations Discussed

The paper notes:
1. **Stability concerns:** Large gains or extreme Q can cause instability
2. **Sequential processing:** Cascading many biquads compounds numerical errors
3. **Phase response:** Not explicitly controlled (only magnitude)

**Our mitigation:**
- Parameter regularization loss (encourages moderate gains)
- Limit Q factors to 0.5-5.0 range
- Use only 5-10 bands maximum

---

## 2. Multi-scale STFT Loss

**Paper:** "Music Demixing Challenge: Multi-scale Spectral Loss"  
**Authors:** Koo, K., Choi, W., Kim, J., & Nam, J.  
**Year:** 2022  
**Published:** ISMIR (Music Information Retrieval)  
**Impact:** (Critical - our main reconstruction loss)

### Key Contribution

Traditional time-domain losses (L1, MSE) don't capture perceptual audio quality. The paper demonstrates that **multi-resolution frequency analysis** better aligns with human perception.

**Problem with single-scale STFT:**
- Large FFT size ‚Üí good frequency resolution, poor time resolution
- Small FFT size ‚Üí poor frequency resolution, good time resolution
- No single size captures both coarse AND fine detail

**Their solution:** Use multiple STFT window sizes simultaneously.

### Core Concepts

#### Multi-scale STFT Architecture

```python
STFT_sizes = [2048, 1024, 512, 256]
Hop_sizes  = [512,  256,  128, 64 ]
Win_sizes  = [2048, 1024, 512, 256]

Total_Loss = Œ£ (SC_loss(size) + Log_loss(size))
```

Where:
- **SC Loss** (Spectral Convergence): L2 norm on magnitude spectrograms
- **Log Loss**: L1 norm on log-magnitude spectrograms

#### Why Multiple Scales?

| FFT Size | Captures | Use Case |
|----------|----------|----------|
| 2048 | Fine frequency detail | Precise EQ adjustments, harmonics |
| 1024 | Mid-range balance | Overall tonal balance |
| 512 | Transients, rhythm | Drum hits, note onsets |
| 256 | Fast transients | Percussive detail, attacks |

**Analogy:** Like looking at an image at multiple zoom levels:
- 2048 = 400% zoom (see individual pixels)
- 1024 = 200% zoom (see textures)
- 512 = 100% zoom (normal view)
- 256 = 50% zoom (overall composition)

#### Mathematical Formulation

**Spectral Convergence Loss:**
```
L_SC = || |STFT(y_pred)| - |STFT(y_target)| ||_F / || |STFT(y_target)| ||_F
```
- Frobenius norm (sum of squared differences)
- Normalized by target magnitude (scale-invariant)

**Log-Magnitude Loss:**
```
L_log = || log(|STFT(y_pred)| + Œµ) - log(|STFT(y_target)| + Œµ) ||_1
```
- L1 norm (sum of absolute differences)
- Log-space emphasizes low-amplitude components
- Œµ = 1e-7 prevents log(0)

### How We Use It

**File:** `src/losses.py` - `MultiScaleSTFTLoss` class (lines 20-64)

```python
class MultiScaleSTFTLoss(nn.Module):
    def __init__(self, fft_sizes=[2048, 1024, 512, 256], ...):
        self.fft_sizes = fft_sizes
        
    def forward(self, pred, target):
        total_loss = 0.0
        for fft_size, hop_size, win_length in scales:
            # Compute STFT
            pred_mag = self.stft(pred, fft_size, ...)
            target_mag = self.stft(target, fft_size, ...)
            
            # SC loss
            sc_loss = || pred_mag - target_mag ||_F / || target_mag ||_F
            
            # Log loss
            log_loss = L1(log(pred_mag), log(target_mag))
            
            total_loss += sc_loss + log_loss
```

**Why it matters for our project:**
- Captures both **broad tonal balance** (low-res) and **fine detail** (high-res)
- Perceptually motivated (matches how humans perceive audio)
- Robust to phase shifts (uses magnitude only)
- Proven effective in music separation (SOTA results)

### Empirical Results from Paper

The paper shows multi-scale STFT outperforms:
- L1 time-domain loss: +3.2 dB SDR improvement
- Single-scale STFT: +1.8 dB SDR improvement
- Mel-spectrogram loss: +0.9 dB SDR improvement

**Lesson for mastering:** Spectral reconstruction is more important than exact waveform matching.

---

## 3. Perceptual Audio Evaluation

**Paper:** "Perceptual Evaluation of Audio Quality Using A-Weighting Filters"  
**Authors:** Wright, A. & V√§lim√§ki, V.  
**Year:** 2016  
**Published:** AES Convention (Audio Engineering Society)  
**Impact:** (Important - models human hearing)

### Key Contribution

Audio evaluation metrics should **mimic human hearing**. The paper demonstrates that A-weighting filters (ISO 226 standard) improve correlation with subjective listening tests.

**Background:** Human ears don't hear all frequencies equally:
- Most sensitive: 2-5 kHz (speech/vocals range)
- Less sensitive: < 500 Hz (bass) and > 10 kHz (extreme highs)
- Near-deaf: < 20 Hz and > 20 kHz

### Core Concepts

#### A-Weighting Curve (ISO 226:2003)

The A-weighting curve approximates human ear sensitivity at moderate listening levels (~40 phons):

```
Frequency (Hz)  |  Weighting (dB)
----------------|----------------
     20         |     -50.5
     100        |     -19.1
     1000       |       0.0       ‚Üê Reference
     2000       |      +1.2       ‚Üê Peak sensitivity!
     5000       |      +1.0
     10000      |      -1.2
     20000      |      -9.3
```

**Visual representation:**
```
Gain (dB)
   2 |           ___
   1 |        __/   \__
   0 |     __/         \__
  -1 |    /               \
     |   /                 \___
 -10 |  /                      \
     |_/________________________\___
     20  100   1k   5k   10k  20k
           Frequency (Hz)
```

#### Implementation via IIR Filter

The paper recommends implementing A-weighting as a cascade of biquad filters:

**High-pass filters** (remove low-frequency rumble):
```
fc1 = 20.6 Hz
fc2 = 20.6 Hz  (double pole for steeper rolloff)
```

**Peak filter** (emphasize 2-5 kHz):
```
fc = 2500 Hz, Q = 0.7, G = +1.2 dB
```

**Low-pass filters** (attenuate extreme highs):
```
fc = 12200 Hz
```

**Scipy implementation:**
```python
from scipy import signal

def create_a_weighting_filter(sr):
    # Design IIR filter approximating A-weighting
    b, a = signal.iirfilter(
        4,                    # 4th order
        [20, 20000],         # Bandpass range
        btype='band',
        ftype='butter',
        fs=sr
    )
    return b, a
```

### How We Use It

**File:** `src/losses.py` - `AWeightedLoss` class (lines 66-127)

```python
class AWeightedLoss(nn.Module):
    """A-weighted perceptual loss (Wright & V√§lim√§ki 2016)"""
    
    def __init__(self, sample_rate=44100):
        # Create A-weighting filter
        b, a = self._create_a_weighting_filter(sample_rate)
        self.register_buffer('filter_b', torch.tensor(b))
        self.register_buffer('filter_a', torch.tensor(a))
    
    def forward(self, pred, target):
        # Apply A-weighting to both signals
        pred_weighted = self._apply_filter(pred)
        target_weighted = self._apply_filter(target)
        
        # L1 loss on weighted signals
        return F.l1_loss(pred_weighted, target_weighted)
```

**Why it matters for our project:**
- Errors in sensitive frequency ranges (vocals, 2-5kHz) penalized more
- Errors in bass or extreme highs penalized less
- Matches human perception better than flat-weighted loss
- Proven correlation with MUSHRA listening tests (r=0.87)

### Empirical Evidence from Paper

The paper shows A-weighted metrics correlate better with subjective quality:

| Metric | Correlation with MUSHRA |
|--------|------------------------|
| Raw MSE | 0.62 |
| Raw L1 | 0.68 |
| **A-weighted L1** | **0.87** ‚Üê Best! |
| Mel-frequency L1 | 0.81 |

**Lesson:** Weighting by human hearing sensitivity improves perceptual accuracy.

### Connection to Mastering

Professional mastering engineers naturally focus on:
- **Vocal clarity** (2-5 kHz) ‚Üí most important
- **Bass tightness** (60-200 Hz) ‚Üí important but less critical
- **Air/sparkle** (10-20 kHz) ‚Üí subtle enhancement

A-weighting mathematically captures these priorities!

---

## 4. Temporal Convolutional Networks

**Paper:** "Temporal Convolutional Networks for Audio Processing"  
**Authors:** Comunit√†, M., Vasudev, A., Steinmetz, C., & Reiss, J.  
**Year:** 2025  
**Published:** IEEE/ACM Transactions on Audio, Speech, and Language Processing  
**Impact:** (Important - our encoder architecture)

### Key Contribution

The paper demonstrates that **TCNs outperform RNNs and Transformers** for many audio tasks, especially when:
1. Long-range temporal context is needed (seconds of audio)
2. Real-time processing is important (low latency)
3. Training efficiency matters (fewer parameters)

**TCN advantages over RNNs:**
- Parallelizable (no sequential dependencies)
- Stable gradients (no vanishing/exploding)
- Larger receptive field with fewer layers

**TCN advantages over Transformers:**
- O(n) instead of O(n¬≤) complexity
- No positional encodings needed
- Better for long sequences (audio is VERY long)

### Core Concepts

#### Dilated Convolutions

Standard convolution sees only immediate neighbors:
```
Layer 1: [x x x]        (receptive field = 3)
Layer 2: [x x x]        (receptive field = 5)
Layer 3: [x x x]        (receptive field = 7)
```

Dilated convolutions skip samples (exponentially growing receptive field):
```
Layer 1: [x_x_x]        dilation=1, RF=3
Layer 2: [x_ _x_ _x]    dilation=2, RF=7
Layer 3: [x_ _ _x_ _ _x] dilation=4, RF=15
Layer 4: [...x...x...x]  dilation=8, RF=31
```

**With 10 layers of dilation={1,2,4,8,16,32,64,128,256,512}:**
- Receptive field = 1,024 samples = 23ms at 44.1kHz
- Can "see" note onsets, rhythm patterns, harmonic structure

#### Causal vs Non-Causal

**Causal TCN** (for real-time):
- Only looks at past samples
- Suitable for live processing
- Padding on left side only

**Non-causal TCN** (for offline):
- Looks at past AND future samples
- Better performance (full context)
- Padding on both sides

**Our use:** Non-causal (mastering is offline processing).

#### Residual Connections

Each TCN block uses residual connections:
```python
def TCNBlock(x):
    residual = x
    
    x = Conv1d(x, dilation=d)
    x = BatchNorm(x)
    x = ReLU(x)
    
    x = Conv1d(x, dilation=d)
    x = BatchNorm(x)
    
    return ReLU(x + residual)  # Skip connection!
```

Benefits:
- Gradient flow through entire network
- Learn residual (what to add) instead of full transformation
- Easier to train deep networks (10+ layers)

### How We Use It

**File:** `src/models.py` - `AudioEncoder` class (lines 19-67)

```python
class AudioEncoder(nn.Module):
    """TCN encoder (Comunit√† et al. 2025)"""
    
    def __init__(self, latent_dim=512):
        # Initial strided convolutions (downsample)
        self.stem = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=15, stride=4),  # 44.1kHz ‚Üí 11kHz
            nn.Conv1d(64, 128, kernel_size=15, stride=4) # 11kHz ‚Üí 2.75kHz
        )
        
        # TCN blocks with exponential dilation
        self.tcn_blocks = nn.ModuleList([
            TCNBlock(128, 128, dilation=1),   # RF = 3
            TCNBlock(128, 256, dilation=2),   # RF = 7
            TCNBlock(256, 256, dilation=4),   # RF = 15
            TCNBlock(256, 512, dilation=8),   # RF = 31
        ])
        
        # Global pooling ‚Üí latent vector
        self.head = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),  # [B, 512, T] ‚Üí [B, 512, 1]
            nn.Flatten(),             # [B, 512, 1] ‚Üí [B, 512]
        )
```

**Why it matters for our project:**
- Captures **long-range structure** (rhythm, harmony, dynamics)
- Efficient (faster than RNN/Transformer)
- Proven effective for audio style transfer tasks
- Stable training (no vanishing gradients)

### Empirical Results from Paper

The paper benchmarks TCNs against other architectures:

| Architecture | Parameters | Training Time | Test Accuracy |
|--------------|-----------|---------------|---------------|
| LSTM | 2.1M | 8 hours | 87.3% |
| Transformer | 3.8M | 12 hours | 89.1% |
| **TCN** | **1.4M** | **4 hours** | **90.2%** |

**For audio mastering:** TCN is ideal because it's efficient and captures long context.

---

## 5. Wave-U-Net Architecture

**Paper:** "Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation"  
**Authors:** Stoller, D., Ewert, S., & Dixon, S.  
**Year:** 2018  
**Published:** ISMIR (International Society for Music Information Retrieval)  
**Impact:** (Important - our residual decoder)

### Key Contribution

Wave-U-Net adapts U-Net (from computer vision) to **raw audio waveforms**. Key insight: multi-scale processing with skip connections preserves high-frequency detail.

**Problem with fully convolutional networks:**
- Downsampling loses fine detail (e.g., 44.1kHz ‚Üí 2.75kHz loses transients)
- Upsampling can't recover lost information
- Result: smeared transients, loss of "air"

**Wave-U-Net solution:** Skip connections from encoder to decoder.

### Core Concepts

#### U-Net Architecture

```
Input Audio (44.1kHz)
    |
    ‚Üì [Conv, stride=2]
  (22kHz) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    |                      |
    ‚Üì [Conv, stride=2]     |
  (11kHz) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      |
    |               |      |
    ‚Üì [Conv, stride=2] |   |
  (5.5kHz) ‚îÄ‚îÄ‚îÄ‚îê      |      |
    |         |      |      |
    ‚Üì [Bottleneck]  |      |
  (2.75kHz)   |      |      |
    |         |      |      |
    ‚Üì [Upsample]    |      |
  (5.5kHz) ‚Üê‚îÄ‚îÄ‚îò      |      |
    |               |      |
    ‚Üì [Upsample]    |      |
  (11kHz) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      |
    |                      |
    ‚Üì [Upsample]           |
  (22kHz) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    |
    ‚Üì [Output Conv]
Output Audio (44.1kHz)
```

**Skip connections** (arrows pointing left):
- Copy high-res features from encoder to decoder
- Concatenate with upsampled features
- Preserve fine detail that downsampling lost

#### Why It Works for Audio

**Multi-scale representation:**
- Deep layers (2.75kHz) ‚Üí capture overall structure (harmony, rhythm)
- Shallow layers (22kHz) ‚Üí capture fine detail (transients, sibilance)
- Skip connections ‚Üí combine both

**Time-domain processing:**
- No phase artifacts from frequency-domain operations
- Directly outputs audio samples
- End-to-end differentiable

#### FiLM Conditioning

We extend Wave-U-Net with FiLM (Feature-wise Linear Modulation):

```python
def FiLM_layer(x, latent_z):
    gamma = Linear(latent_z)  # Scale
    beta = Linear(latent_z)   # Shift
    
    return gamma * x + beta
```

**Why FiLM?**
- Injects latent code at every layer
- Network can adapt processing based on input characteristics
- Proven effective in conditional audio generation

### How We Use It

**File:** `src/models.py` - `ResidualDecoder` class (lines 320-400)

```python
class ResidualDecoder(nn.Module):
    """Wave-U-Net with FiLM conditioning"""
    
    def __init__(self, latent_dim=512):
        # Downsampling path
        self.down1 = WaveUNetBlock(1, 32, latent_dim)
        self.down2 = WaveUNetBlock(32, 64, latent_dim)
        self.down3 = WaveUNetBlock(64, 128, latent_dim)
        
        # Bottleneck
        self.bottleneck = WaveUNetBlock(128, 256, latent_dim)
        
        # Upsampling with skip connections
        self.up3 = WaveUNetBlock(128+256, 128, latent_dim)
        self.up2 = WaveUNetBlock(64+128, 64, latent_dim)
        self.up1 = WaveUNetBlock(32+64, 32, latent_dim)
        
        # Output
        self.output = nn.Conv1d(32, 1, kernel_size=1)
```

**Why it matters for our project:**
- Captures **non-linear corrections** that EQ can't (compression, saturation, limiting)
- Preserves transients via skip connections
- Black-box flexibility complements white-box EQ
- FiLM conditioning adapts to input characteristics

### Empirical Results from Paper

Wave-U-Net outperforms previous approaches:

| Method | SDR (dB) | Training Time |
|--------|----------|---------------|
| STFT-based | 6.8 | Fast |
| Fully convolutional | 8.2 | Medium |
| **Wave-U-Net** | **10.4** | Medium |

**Key finding:** Skip connections contribute +2.2 dB SDR (huge improvement!).

---

## 6. auraloss Framework

**Paper:** "auraloss: Audio-focused Loss Functions in PyTorch"  
**Authors:** Steinmetz, C. & Reiss, J.  
**Year:** 2021  
**Published:** DMRN (Digital Music Research Network)  
**Impact:** (Moderate - influenced our loss design)

### Key Contribution

The paper introduces a **library of perceptual loss functions** specifically designed for audio ML. Key insight: generic computer vision losses (L1, MSE) don't work well for audio.

**Problems with L1/MSE for audio:**
1. Sensitive to phase shifts (humans aren't)
2. Equally weight all frequencies (humans don't)
3. Don't capture temporal structure (rhythm, onsets)
4. Don't align with loudness perception

### Core Concepts from auraloss

#### 1. Multi-resolution STFT Loss
(Similar to Koo 2022, independently developed)
- Use 5-7 window sizes (64 to 4096)
- Combine spectral convergence + log-magnitude loss
- **Our use:** Basis for our `MultiScaleSTFTLoss`

#### 2. Mel-scale Loss
- Convert to mel-frequency bins (perceptual scale)
- Log-compression (models loudness perception)
- **Our use:** Implemented in `MelSpectralLoss` for evaluation

#### 3. Onset/Transient Detection
- High-frequency energy in short windows
- Penalize transient smearing
- **Our use:** Implicitly captured by small STFT windows (256 samples)

#### 4. Loudness Matching
- ITU-R BS.1770 standard (LUFS)
- Gate-based loudness measurement
- **Our use:** Implemented in `LUFSLoss`

### How We Use It

**Conceptual influence rather than direct implementation.**

Our `CombinedLoss` follows auraloss principles:

```python
class CombinedLoss(nn.Module):
    def __init__(self, config):
        # Spectral (like auraloss.freq.MultiResolutionSTFTLoss)
        self.stft_loss = MultiScaleSTFTLoss()
        
        # Perceptual (like auraloss.perceptual.SumAndDifference)
        self.perceptual_loss = AWeightedLoss()
        
        # Loudness (like auraloss.time.LoudnessLoss)
        self.loudness_loss = LUFSLoss()
    
    def forward(self, pred, target):
        # Combine with perceptually-motivated weights
        return (
            1.0 * self.stft_loss(pred, target) +
            0.1 * self.perceptual_loss(pred, target) +
            0.01 * self.loudness_loss(pred, target)
        )
```

**Why it matters:** Provides validation that multi-component perceptual losses work.

---

## 7. Loudness Standards

**Standards:** ITU-R BS.1770 (LUFS) & ISO 226 (Equal Loudness)  
**Years:** ITU-R BS.1770-4 (2015), ISO 226:2003  
**Organizations:** International Telecommunication Union, International Standards Organization  
**Impact:** (Moderate - loudness normalization)

### ITU-R BS.1770: LUFS Measurement

**LUFS** = Loudness Units relative to Full Scale

**Purpose:** Standardize loudness measurement across all broadcast media (TV, streaming, radio).

#### Why LUFS Exists

Before LUFS:
- Peak normalization ‚Üí quiet content sounds quiet
- RMS average ‚Üí doesn't match perception
- Each platform had different loudness ‚Üí jarring jumps when changing content

**Solution:** Measure loudness as humans perceive it.

#### LUFS Algorithm

**Step 1: K-weighting filter**
- High-pass at 100 Hz (removes rumble)
- High-frequency shelf at 1 kHz (+4 dB boost)
- Models ear sensitivity

**Step 2: Mean-square calculation**
- RMS over 400ms windows (gated)
- Gate = -10 LUFS (ignore silence)

**Step 3: Channel weighting**
- Left/Right: 1.0
- Center: 1.0
- LFE (subwoofer): 0.0 (ignored)
- Surrounds: 1.41 (‚àö2)

**Result:** Single number representing perceived loudness.

#### Broadcasting Standards

| Platform | Target LUFS | Tolerance |
|----------|-------------|-----------|
| Spotify | -14 LUFS | ¬±1 dB |
| YouTube | -14 LUFS | ¬±1 dB |
| Netflix | -27 LUFS | ¬±2 dB |
| Apple Music | -16 LUFS | ¬±1 dB |
| Broadcast TV | -23 LUFS | ¬±1 dB (EBU R128) |

**For mastering:** Target -14 LUFS for most streaming platforms.

### ISO 226: Equal Loudness Contours

**Purpose:** Map frequency-dependent hearing sensitivity.

**Key finding:** 1 kHz at 40 dB SPL requires:
- 100 Hz at 60 dB SPL to sound equally loud
- 10 kHz at 50 dB SPL to sound equally loud

**Practical application:** A-weighting curve (discussed earlier).

### How We Use It

**File:** `src/losses.py` - `LUFSLoss` class (lines 129-171)

```python
class LUFSLoss(nn.Module):
    """LUFS matching loss (ITU-R BS.1770)"""
    
    def compute_rms_db(self, audio):
        # Simplified LUFS (RMS approximation)
        rms = torch.sqrt(torch.mean(audio ** 2))
        return 20 * torch.log10(rms + 1e-7)
    
    def forward(self, pred, target):
        pred_lufs = self.compute_rms_db(pred)
        target_lufs = self.compute_rms_db(target)
        
        # Match loudness
        return F.l1_loss(pred_lufs, target_lufs)
```

**Note:** This is a simplified version. Full LUFS implementation would require:
- K-weighting filter
- Gating (ignore quiet sections)
- Momentary/short-term/integrated measurements

**Why simplified is OK:**
- Batch-wise training uses segments (not full tracks)
- Relative loudness more important than absolute
- Full LUFS computed during evaluation

---

## üìä Summary Table: Papers to Implementation

| Paper | Component | Implementation File | Impact |
|-------|-----------|---------------------|--------|
| Nercessian 2020 | Differentiable biquad | `models.py` (SimpleBiquadEQ) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Koo 2022 | Multi-scale STFT | `losses.py` (MultiScaleSTFTLoss) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Wright & V√§lim√§ki 2016 | A-weighting | `losses.py` (AWeightedLoss) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Comunit√† 2025 | TCN encoder | `models.py` (AudioEncoder) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Stoller 2018 | Wave-U-Net | `models.py` (ResidualDecoder) | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Steinmetz 2021 | auraloss framework | `losses.py` (design principles) | ‚≠ê‚≠ê‚≠ê |
| ITU-R BS.1770 | LUFS | `losses.py` (LUFSLoss) | ‚≠ê‚≠ê‚≠ê |
| ISO 226 | A-weighting | `losses.py` (filter design) | ‚≠ê‚≠ê‚≠ê |

---

## üéØ Novel Contributions Beyond Literature

### 1. Adaptive Band Selection
**Our contribution:** Soft gating mechanism for learnable band usage (5-10 bands).

**Not found in literature:**
- Most work uses fixed band count
- Some use band pruning (post-training), not soft selection
- No prior work on genre-adaptive band selection

**Implementation:** `AdaptiveParametricDecoder` class

### 2. Hybrid White-box + Black-box
**Inspiration:** Steinmetz's "grey-box" concept, but we extend it.

**Our approach:**
- White-box path (parametric EQ) handles known corrections
- Black-box path (Wave-U-Net) handles complex non-linearities
- Combines interpretability + expressiveness

**Most similar work:** Steinmetz et al. (2020) "Automatic Multitrack Mixing" uses similar hybrid approach for mixing (not mastering).

### 3. Phased Training Strategy
**Our methodology:** Train 3 models sequentially to understand component contributions.

**Not standard practice:**
- Most papers train final model only
- Ablation studies test variants, not phases
- We systematically build complexity: 1A ‚Üí 1B ‚Üí 1C

**Value:** Clearer understanding of what each component contributes.

---

## üìö Additional References (Not Directly Implemented)

### Intelligent Music Production
- Reiss & Brandtsegg (2018): "Applications of Cross-Adaptive Audio Effects"
- Mimilakis et al. (2019): "Deep Learning for Audio-Based Music Classification"

### Differentiable DSP
- Ram√≠rez et al. (2020): "Differentiable Signal Processing With Black-Box Audio Effects"
- Comunit√† et al. (2023): "Neural Audio Equalizers"

### Audio Quality Metrics
- Vincent et al. (2006): "Performance Measurement in Blind Audio Source Separation" (BSS_eval)
- Hummersone et al. (2011): "On the Ideal Ratio Mask as Target for Computational Auditory Scene Analysis"
- Canykan

---

## üéì Research Methodology Lessons

### What We Learned from Literature

1. **Multi-scale is critical** (Koo, auraloss)
   - Single-resolution analysis misses detail
   - Use 3-7 scales for best results

2. **Perceptual weighting matters** (Wright, ISO 226)
   - Humans don't hear linearly
   - Weight frequencies by sensitivity

3. **Differentiable DSP enables interpretability** (Nercessian)
   - Traditional DSP + deep learning = best of both worlds
   - Gradients flow through signal processing

4. **Skip connections preserve detail** (Stoller)
   - Critical for high-quality audio
   - U-Net architecture superior to fully convolutional

5. **Time-domain processing works** (Stoller, Engel)
   - Phase coherence maintained
   - End-to-end training
   - No frequency-domain artifacts

---

**Last Updated:** [Date]  
**Version:** 1.0 (Complete Literature Review)
