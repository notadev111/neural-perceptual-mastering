# Project Structure

Complete file organization for the Neural Perceptual Audio Mastering system.

```
neural-mastering/
â”‚
â”œâ”€â”€ README.md                          # Comprehensive project documentation
â”œâ”€â”€ QUICKSTART.md                      # Quick start guide for getting started
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .gitignore                         # Git ignore patterns
â”‚
â”œâ”€â”€ configs/                           # Training configurations
â”‚   â”œâ”€â”€ phase1a_parametric_only.yaml   # Phase 1A: Parametric EQ only
â”‚   â”œâ”€â”€ phase1b_hybrid.yaml            # Phase 1B: EQ + Residual
â”‚   â””â”€â”€ phase1c_adaptive.yaml          # Phase 1C: Adaptive bands (novel)
â”‚
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ __init__.py                    # Package initialization
â”‚   â”œâ”€â”€ models.py                      # Model architectures
â”‚   â”‚   â”œâ”€â”€ AudioEncoder               # TCN encoder
â”‚   â”‚   â”œâ”€â”€ ParametricDecoder          # Differentiable EQ
â”‚   â”‚   â”œâ”€â”€ AdaptiveParametricDecoder  # Adaptive bands (novel)
â”‚   â”‚   â”œâ”€â”€ ResidualDecoder            # Wave-U-Net residual
â”‚   â”‚   â”œâ”€â”€ MasteringModel_Phase1A     # Phase 1A model
â”‚   â”‚   â”œâ”€â”€ MasteringModel_Phase1B     # Phase 1B model
â”‚   â”‚   â””â”€â”€ MasteringModel_Phase1C     # Phase 1C model
â”‚   â”‚
â”‚   â”œâ”€â”€ losses.py                      # Loss functions
â”‚   â”‚   â”œâ”€â”€ MultiScaleSTFTLoss         # Multi-resolution spectral loss
â”‚   â”‚   â”œâ”€â”€ AWeightedLoss              # A-weighted perceptual loss ***YOUR FUNCTION***
â”‚   â”‚   â”œâ”€â”€ LUFSLoss                   # Loudness matching
â”‚   â”‚   â”œâ”€â”€ ParameterRegularizationLoss# EQ parameter regularization
â”‚   â”‚   â”œâ”€â”€ MelSpectralLoss            # Mel-frequency spectral distance
â”‚   â”‚   â””â”€â”€ CombinedLoss               # Weighted combination
â”‚   â”‚
â”‚   â”œâ”€â”€ data_loader.py                 # Dataset and data loading
â”‚   â”‚   â”œâ”€â”€ MasteringDataset           # Pre/post mastering pairs
â”‚   â”‚   â”œâ”€â”€ get_dataloaders            # Create train/val/test loaders
â”‚   â”‚   â””â”€â”€ create_dummy_dataset       # Generate test data
â”‚   â”‚
â”‚   â”œâ”€â”€ train.py                       # Training script
â”‚   â”‚   â”œâ”€â”€ train_epoch()              # Train one epoch
â”‚   â”‚   â”œâ”€â”€ validate()                 # Validation
â”‚   â”‚   â””â”€â”€ save_checkpoint()          # Save model
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluate.py                    # Evaluation and metrics
â”‚   â”‚   â”œâ”€â”€ compute_snr()              # Signal-to-noise ratio
â”‚   â”‚   â”œâ”€â”€ compute_mel_distance()     # Mel spectral distance
â”‚   â”‚   â”œâ”€â”€ analyze_eq_parameters()    # EQ parameter analysis
â”‚   â”‚   â”œâ”€â”€ visualize_eq_curve()       # Plot frequency response
â”‚   â”‚   â””â”€â”€ evaluate_model()           # Comprehensive evaluation
â”‚   â”‚
â”‚   â””â”€â”€ inference.py                   # Process audio through model
â”‚       â”œâ”€â”€ load_model()               # Load trained model
â”‚       â”œâ”€â”€ preprocess_audio()         # Load and normalize audio
â”‚       â”œâ”€â”€ process_audio()            # Process in segments
â”‚       â””â”€â”€ postprocess_audio()        # Save output
â”‚
â”œâ”€â”€ data/                              # Dataset (user-provided)
â”‚   â”œâ”€â”€ unmastered/                    # Pre-mastering audio
â”‚   â”‚   â”œâ”€â”€ song001.wav
â”‚   â”‚   â”œâ”€â”€ song002.wav
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ mastered/                      # Post-mastering audio (targets)
â”‚       â”œâ”€â”€ song001.wav
â”‚       â”œâ”€â”€ song002.wav
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ checkpoints/                       # Saved model checkpoints (generated)
â”‚   â”œâ”€â”€ phase_1A/
â”‚   â”‚   â”œâ”€â”€ best_model.pt
â”‚   â”‚   â””â”€â”€ phase_1A_epoch_*.pt
â”‚   â”œâ”€â”€ phase_1B/
â”‚   â””â”€â”€ phase_1C/
â”‚
â””â”€â”€ runs/                              # Tensorboard logs (generated)
    â”œâ”€â”€ phase_1A_20250101_120000/
    â”œâ”€â”€ phase_1B_20250108_120000/
    â””â”€â”€ phase_1C_20250115_120000/
```

## ðŸ“ Key File Details

### Configuration Files (`configs/`)
YAML files defining model architecture, hyperparameters, and training settings for each phase.

### Source Code (`src/`)

#### `models.py` (521 lines)
- **AudioEncoder**: TCN-based encoder with dilated convolutions
- **ParametricDecoder**: MLP â†’ differentiable biquad EQ (white-box)
- **AdaptiveParametricDecoder**: Novel adaptive band selection
- **ResidualDecoder**: Wave-U-Net with FiLM conditioning (black-box)
- **Phase Models**: 1A (parametric only), 1B (hybrid), 1C (adaptive)

#### `losses.py` (305 lines) **â† YOUR A-WEIGHTED LOSS IS HERE**
- **MultiScaleSTFTLoss**: Captures fine and coarse spectral detail
- **AWeightedLoss**: Perceptual loss using ISO 226 A-weighting filter
- **LUFSLoss**: Loudness matching (ITU-R BS.1770)
- **ParameterRegularizationLoss**: Encourages minimal EQ adjustments
- **MelSpectralLoss**: Mel-frequency perceptual metric
- **CombinedLoss**: Weighted combination of all losses

#### `data_loader.py` (210 lines)
- **MasteringDataset**: Loads pre/post mastering pairs
- Self-supervised segmentation (5s chunks)
- Audio augmentation (random gain, polarity flip)
- Automatic mono conversion and resampling

#### `train.py` (242 lines)
- Phase-aware model selection
- Training loop with validation
- Tensorboard logging
- Checkpoint saving
- Learning rate scheduling

#### `evaluate.py` (329 lines)
- Multiple perceptual metrics (STFT, mel distance, SNR, LUFS, A-weighted)
- EQ parameter analysis
- Frequency response visualization
- Metric distribution plots

#### `inference.py` (248 lines)
- Load trained model
- Process long audio files in segments
- Extract and display EQ parameters
- Save mastered output

## ðŸŽ¯ Training Workflow

```
Phase 1A (Baseline)
â””â”€> configs/phase1a_parametric_only.yaml
    â””â”€> src/train.py
        â””â”€> checkpoints/phase_1A/best_model.pt
            â””â”€> src/evaluate.py
                â””â”€> results_phase1a/

Phase 1B (Hybrid)
â””â”€> configs/phase1b_hybrid.yaml
    â””â”€> src/train.py
        â””â”€> checkpoints/phase_1B/best_model.pt
            â””â”€> src/evaluate.py
                â””â”€> results_phase1b/

Phase 1C (Novel)
â””â”€> configs/phase1c_adaptive.yaml
    â””â”€> src/train.py
        â””â”€> checkpoints/phase_1C/best_model.pt
            â””â”€> src/evaluate.py
                â””â”€> results_phase1c/
```

## ðŸ“Š Generated Outputs

### During Training:
- `runs/` - Tensorboard logs (loss curves, parameter tracking)
- `checkpoints/` - Model weights saved every N epochs

### During Evaluation:
- `evaluation_results/phase_X_metrics.json` - Numerical results
- `evaluation_results/phase_X_eq_curve.png` - Average EQ frequency response
- `evaluation_results/phase_X_metrics.png` - Metric distribution histograms

### During Inference:
- `audio/*_mastered.wav` - Processed audio outputs

## ðŸ” Where to Find Key Components

| What | Where |
|------|-------|
| **Your A-weighted loss function** | `src/losses.py` line 66-127 |
| **Differentiable EQ** | `src/models.py` line 69-120 |
| **TCN encoder** | `src/models.py` line 19-67 |
| **Wave-U-Net residual** | `src/models.py` line 320-400 |
| **Phase 1A model** | `src/models.py` line 410-447 |
| **Phase 1B model** | `src/models.py` line 450-495 |
| **Phase 1C model** | `src/models.py` line 498-544 |
| **Training loop** | `src/train.py` line 22-88 |
| **Evaluation metrics** | `src/evaluate.py` line 18-160 |
| **Data loading** | `src/data_loader.py` line 18-190 |

## ðŸš€ Quick Commands

```bash
# Train Phase 1A
python src/train.py --config configs/phase1a_parametric_only.yaml

# Evaluate Phase 1A
python src/evaluate.py \
    --checkpoint checkpoints/phase_1A/best_model.pt \
    --config configs/phase1a_parametric_only.yaml \
    --save_dir evaluation_results

# Process audio
python src/inference.py \
    --checkpoint checkpoints/phase_1A/best_model.pt \
    --config configs/phase1a_parametric_only.yaml \
    --input audio/unmastered.wav \
    --output audio/mastered.wav

# Monitor training
tensorboard --logdir runs/
```

## ðŸ“¦ Dependencies

Install via `requirements.txt`:
- `torch>=2.0.0` - Deep learning framework
- `torchaudio>=2.0.0` - Audio processing
- `librosa>=0.10.0` - Audio analysis
- `tensorboard>=2.13.0` - Training visualization
- `matplotlib>=3.7.0` - Plotting
- `scipy>=1.10.0` - Scientific computing
- `pyyaml>=6.0` - Config files

## ðŸ”® Future Extensions (Phase 2)

The modular structure allows easy addition of:
- Differentiable compressor in `src/models.py`
- Differentiable saturation/distortion
- Stereo imaging module
- Dynamic limiting
- Text-conditioning (CLAP embeddings)

All would be added to `ResidualDecoder` or a new `GreyBoxDecoder` class.

---

**Version:** 1.0 (Phases 1A-C)  
**Last Updated:** [Date]
